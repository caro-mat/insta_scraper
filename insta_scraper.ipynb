{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREDITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "creds = dict() # dictionary to hold everything\n",
    "#creds['access_token'] = 'EAAmFZATZCLgxgBAGUYzNF08WPhKdW2c0ZAhM4p2qyo8VdZCf2hmXoTU424DyVmsfB7lPgQbBeAhGpWrUgsDRSrUhkITXf3qb0fCljjPwy1Eug41n3cCuvfhni26qV9ZBqHLZA4daz4A85G40OWcmgRmFZC3npHF2r5RIE0ijN1ZCHQZDZD' # access token for use with all api calls\n",
    "#creds['client_id'] = '2679944698888984' # client id from facebook app IG Graph API Test\n",
    "#creds['client_secret'] = '390ee574a4874770353dc3ad66ed988c' # client secret from facebook app\n",
    "creds['graph_domain'] = 'https://graph.facebook.com/' # base domain for api calls\n",
    "creds['graph_version'] = 'v7.0' # version of the api we are hitting\n",
    "creds['endpoint_base'] = creds['graph_domain'] + creds['graph_version'] + '/' # base endpoint with domain and version\n",
    "creds['debug'] = 'no' # debug mode for api call\n",
    "creds['page_id'] = '107303807767474' # users page id\n",
    "creds['instagram_account_id'] = '17841440887073232' # users instagram account id\n",
    "creds['ig_username'] = 'brandselfies_business' # ig username\n",
    "\n",
    "#backup brandselfies2\n",
    "creds['access_token'] = \"EAAEqg3ZAgchoBACz4xLWZCRc311NTVOB8SCAMkCYYpj0gYOLTzyAWOFDdDNjnG9fZCxxLCSBU5ObMFtrCaS0oIwyXZADOsq5Dm6LZCsz41YZACbO4xfYtSCUc21NQZBjSrqfc1G2xCeeH54lrZCnLMMn1WM9kYTTKVQZD\"\n",
    "creds['client_id'] = '328219091825178' # client id from facebook app IG Graph API Test\n",
    "creds['client_secret'] = 'c3ec335993b83d2abbdf9d828ed76219' # client secret from facebook app\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH BY HASHTAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Hashtag and get Hashtag ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "myhashtag = \"selfie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Specify fields to scrape \n",
    "fields = \"id,name\"\n",
    "\n",
    "#Specify url\n",
    "url = \"https://graph.facebook.com/v8.0/ig_hashtag_search?user_id=\" + creds['instagram_account_id']+ \"&q=\" + myhashtag +\"&fields=\" + fields + \"&access_token=\" + creds['access_token']\n",
    "\n",
    "#make request and get data \n",
    "data = requests.get( url) # make get request\n",
    "json_data = json.loads( data.content ) # response data from the api\n",
    "hashtag_id = json_data[\"data\"][0][\"id\"]\n",
    "\n",
    "print(\"The hashtag is:\")\n",
    "print(myhashtag)\n",
    "\n",
    "print(\"The hashtag_id is:\")\n",
    "print(hashtag_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data by hashtag ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#https://graph.facebook.com/v8.0/{ig-hashtag-id}/top_media?user_id={user-id}&fields={fields}\n",
    "\t\n",
    "fields = 'id,children,caption,comment_count,like_count,media_type,media_url,permalink,timestamp' # fields to get back\n",
    "\n",
    "url = \"https://graph.facebook.com/v8.0/\" + hashtag_id + '/top_media?user_id=' + creds['instagram_account_id']+ \"&fields=\" + fields + \"&access_token=\" + creds['access_token']\n",
    "response = []\n",
    "x = 0\n",
    "\n",
    "num_pages = 10 #number of pages to scrape\n",
    "\n",
    "for x in range (0,num_pages):\n",
    "  try:\n",
    "    print(str(x+1) + \".page is being scraped with url \" + str(url))\n",
    "    data = requests.get(url) # make get request\n",
    "    json_data = json.loads( data.content ) # response data from the api\n",
    "    response.append(json_data)\n",
    "    url = json_data[\"paging\"][\"next\"]\n",
    "    x = x+1\n",
    "  except:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "posts = []\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(0,num_pages) :\n",
    "  for post in response[i]['data'] : \n",
    "\n",
    "    try:\n",
    "      id = (post['id'])\n",
    "    except:\n",
    "      id = \"\"\n",
    "\n",
    "    try:\n",
    "      permalink = (post['permalink'])\n",
    "    except:\n",
    "      permalink = \"\"\n",
    "    \n",
    "    try:\n",
    "      media_url = post['media_url'] \n",
    "    except: \n",
    "      media_url = \"\"\n",
    "\n",
    "    try:\n",
    "      caption = post['caption'] \n",
    "    except: \n",
    "      caption = \"\"\n",
    "\n",
    "    try:\n",
    "      media_type = post['media_type'] \n",
    "    except: \n",
    "      media_type = \"\"\n",
    "\n",
    "    try:\n",
    "      like_count = post['like_count'] \n",
    "    except: \n",
    "      like_count = \"\"\n",
    "\n",
    "    try:\n",
    "      timestamp = post['timestamp'] \n",
    "    except: \n",
    "      timestamp = \"\"\n",
    "    \n",
    "    post = [id, permalink, media_url, caption, media_type, like_count, timestamp]\n",
    "    #print(post)\n",
    "    posts.append(post)\n",
    "\n",
    "  i = i + 1\n",
    "  print(\"i is now\" + str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "posts_data_frame = pd.DataFrame(posts)\n",
    "posts_data_frame.columns = [\"id\", \"permalink\", \"media_url\", \"caption\", \"media_type\", \"like_count\", \"timestamp\"]\n",
    "print(posts_data_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "full_path = \"instagram_data/data-20-09-14-hashtag-selfie.csv\"\n",
    "\n",
    "\n",
    "with open(full_path, 'w', encoding='utf-8', newline='') as file:\n",
    "  export = posts_data_frame.to_csv(quoting=csv.QUOTE_NONNUMERIC, index=False, encoding='utf-8')\n",
    "  file.write(export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH BY USER PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name_search = \"beerselfie\"\n",
    "#user_id_search = creds['instagram_account_id'] #my own account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graph.facebook.com/{graph-api-version}/{ig-user-id}?fields=business_discovery.username({ig-username}){username,website,name,ig_id,id,profile_picture_url,biography,follows_count,followers_count,media_count}&access_token={access-token}\n",
    "\n",
    "url = \"https://graph.facebook.com/v8.0/\" + creds['instagram_account_id'] + \"?fields=business_discovery.username(\" + user_name_search + \"){username,website,name,ig_id,id,profile_picture_url,biography,follows_count,followers_count,media_count}&access_token=\" + creds['access_token']\n",
    "\n",
    "print(url)\n",
    "\n",
    "data = requests.get(url) # make get request\n",
    "json_data = json.loads( data.content ) # response data from the api\n",
    "\n",
    "print(\"username:\")\n",
    "print(json_data[\"business_discovery\"][\"username\"])\n",
    "print(\"\\nuser_id:\")\n",
    "print(json_data[\"business_discovery\"][\"ig_id\"])\n",
    "print(\"\\nfollows_count:\")\n",
    "print(json_data[\"business_discovery\"][\"follows_count\"])\n",
    "print(\"\\nfollowers_count:\")\n",
    "print(json_data[\"business_discovery\"][\"followers_count\"])\n",
    "print(\"\\nmedia_count\")\n",
    "print(json_data[\"business_discovery\"][\"media_count\"])\n",
    "\n",
    "user_id_search = json_data[\"business_discovery\"][\"ig_id\"]\n",
    "print(\"\\nthe user id \" + str(user_id_search) + \" of the user \" + user_name_search + \" is now stored in the variable user_id_search \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of new access tokens:\n",
    "\n",
    "def getNewAccessToken(token):\n",
    "  mytokens = [\"EAAEqg3ZAgchoBAGxW6z5YY3MzsChoQKgpP0d8CK13ilLZA5DcuEBcyeZAneS6QNnZCTtZAZAKjlAakZChZCXak0zKkIE0qZAKkcfWHA4y7xUNsjPTNOvZCCwHeDOx1FyQdYnYZBhIEEQHE2SlP0bkaxLjm9rhFiKFYKSOStLFnvdslkCQyZCcqZAhahMzDvg5qqo27X8ZD\",\n",
    "              2,\n",
    "              3,\n",
    "              4,\n",
    "              5]\n",
    "    \n",
    "  creds['access_token'] = mytokens[token]\n",
    "  return creds['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = 'caption,media_url,media_type,like_count,comments_count,id,timestamp,permalink' # fields for first url -> EDIT DIRECTLY IN LOOP FOR PAGES 2 AND AFTER\n",
    "\n",
    "response = []\n",
    "\n",
    "#starting_url_vorlage = graph.facebook.com/17841440887073232?fields=business_discovery.username(beerselfie){media{caption,media_url,media_type,like_count,comments_count,id}}&access_token=EAAEqg3ZAgchoBADqSp7acTrdtmBtLvsTzZCWoVa2MUYC8Q5B5GotW5fclR4xlQbn76MLjWqGWgxNWslZBafXPKWrM6Jj7YrIjK7B1GzBs76scZCLUVcHzMrW9H7WZBOWHXcACYLaxZAdhi9ztEuYZAG2fdOOz7tP8lZBCbfhZBhm0QZBtfBRRGsGNEIW9mPmkiOYUZD\n",
    "#paging_url = \"https://graph.facebook.com/v2.12/\" + creds['instagram_account_id'] + \"?fields=business_discovery.username(\" + user_name_search +\")\"+ \"{id,name,username,website,profile_picture_url,biography,followers_count,media_count,media.after\" +\"(\"+after+\"){id,caption,comments_count,like_count,media_type,media_url,owner,timestamp}}&access_token=\"+creds['access_token']\n",
    "\n",
    "starting_url = \"https://graph.facebook.com/\" + creds['instagram_account_id'] + \"?fields=business_discovery.username(\" + user_name_search +\"){media{\" + fields + \"}}&access_token=\" + creds['access_token']\n",
    "url = starting_url\n",
    "i = 0\n",
    "max_pages = 498\n",
    "token = 0\n",
    "\n",
    "for i in range (0,max_pages):\n",
    "  try: \n",
    "    print(\"Page\" + str(i) + \" with url \" + url + \" is being scraped\")\n",
    "    data = requests.get(url) # make get request\n",
    "    json_data = json.loads( data.content ) # response data from the api\n",
    "    response.append(json_data)\n",
    "    after = json_data[\"business_discovery\"][\"media\"][\"paging\"][\"cursors\"][\"after\"]\n",
    "    url = \"https://graph.facebook.com/v2.12/\" + creds['instagram_account_id'] + \"?fields=business_discovery.username(\" + user_name_search +\")\"+ \"{id,name,username,website,profile_picture_url,biography,followers_count,media_count,media.after\" +\"(\"+after+\"){id,caption,comments_count,like_count,media_type,media_url,owner,timestamp,permalink}}&access_token=\"+creds['access_token']\n",
    "    i = i+1\n",
    "    print(\"after code: \" + str(after))\n",
    "  except:\n",
    "    print(\"Something went wrong, trying again with new access token\")\n",
    "    getNewAccessToken(token)\n",
    "    token = token + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "posts = []\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(0,max_pages) :\n",
    "  for post in response[i][\"business_discovery\"][\"media\"][\"data\"] : \n",
    "\n",
    "    try:\n",
    "      id = (post['id'])\n",
    "    except:\n",
    "      id = \"\"\n",
    "\n",
    "    try:\n",
    "      permalink = (post['permalink'])\n",
    "    except:\n",
    "      permalink = \"\"\n",
    "    \n",
    "    try:\n",
    "      media_url = post['media_url'] \n",
    "    except: \n",
    "      media_url = \"\"\n",
    "\n",
    "    try:\n",
    "      caption = post['caption'] \n",
    "    except: \n",
    "      caption = \"\"\n",
    "\n",
    "    try:\n",
    "      media_type = post['media_type'] \n",
    "    except: \n",
    "      media_type = \"\"\n",
    "\n",
    "    try:\n",
    "      like_count = post['like_count'] \n",
    "    except: \n",
    "      like_count = \"\"\n",
    "\n",
    "    try:\n",
    "      timestamp = post['timestamp'] \n",
    "    except: \n",
    "      timestamp = \"\"\n",
    "    \n",
    "    post = [id, permalink, media_url, caption, media_type, like_count, timestamp]\n",
    "    #print(post)\n",
    "    posts.append(post)\n",
    "\n",
    "  i = i + 1\n",
    "  print(\"i is now\" + str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "path = \"instagram_data\"\n",
    "filename = \"data-20-09-14-profile-beerselfie2\"\n",
    "fileformat = \".csv\"\n",
    "nr = 0\n",
    "\n",
    "full_path = path + \"/\"+ filename + str(nr) + fileformat\n",
    "print(full_path)\n",
    "\n",
    "\n",
    "with open(full_path, 'w', encoding='utf-8', newline='') as file:\n",
    "  export = posts_data_frame.to_csv(quoting=csv.QUOTE_NONNUMERIC, index=False, encoding='utf-8')\n",
    "  file.write(export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET PICTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "path_pic = \"instagram_data/images/\"\n",
    "\n",
    "\n",
    "for index, row in posts_data_frame.iterrows():\n",
    "  try:\n",
    "    urllib.request.urlretrieve(row['media_url'], path_pic+str(row['id'])+\".jpg\" )\n",
    "  except:\n",
    "    print(\"post with id \" + row['id'] + \" could not be saved\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
